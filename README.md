# Fine-Tuning vs. Few-Shot: Evaluating Flan-T5 and LLaMA for Commonsense Reasoning Explanations

This repository contains code and results for a research study investigating the performance of fine-tuned smaller AI models versus few-shot larger models in generating explanations for commonsense reasoning tasks. The study explores whether smaller, fine-tuned models can provide explanations comparable in quality to those generated by larger, more resource-intensive models.

## Project Overview

This research compares two distinct modeling approaches for commonsense reasoning explanation generation:

### Models Evaluated:
- **Flan-T5 Base** (~250M parameters) - Fine-tuned encoder-decoder model optimized for instruction-following tasks
- **LLaMA 7B** (7B parameters) - Large language model using few-shot prompting approach
- **DeBERTaV3 Large** - Used as an automated evaluation model to assess explanation quality

### Research Question:
Can smaller, fine-tuned models generate explanations for commonsense reasoning tasks that are comparable in quality to those produced by larger, few-shot models?

### Key Focus Areas:
- **Fine-tuning vs. Few-shot learning** - Comparing targeted training versus in-context learning
- **Explanation quality assessment** - Multi-dimensional evaluation of generated explanations
- **Human-AI evaluation alignment** - Agreement between human ratings and automated metrics
- **Resource efficiency** - Balancing model performance with computational requirements

## Repository Structure

```
Code/
├── 100_FINAL.csv              # Final evaluation results for 100-sample subset
├── 500_FINAL.csv              # Final evaluation results for 500-sample subset
├── results_labelAccuracy.csv  # Label accuracy comparison across models
├── plotCode/                  # Data visualization and analysis
│   ├── comparingFewShots.ipynb           # Few-shot comparison analysis
│   ├── evaluation100.ipynb               # 100-sample evaluation plots
│   ├── evaluation500.ipynb               # 500-sample evaluation plots
│   ├── plot100.py                        # Plotting script for 100-sample results
│   ├── plot500.py                        # Plotting script for 500-sample results
│   ├── Generated_vs_Human1_Kendalls_Tau.png
│   ├── Human_Kendalls_Tau.png
│   ├── LLaMA_Generated_vs_Human_Kendalls_Tau.png
│   ├── T5_Generated_vs_Human_Kendalls_Tau.png
│   └── LabelAccuracycomparingFewShots.png
└── theCode/                   # Main experimental code
    ├── ACORN.jsonl                       # ACORN dataset (3,500 explanations)
    ├── ACORNviewer.ipynb                 # Dataset exploration notebook
    ├── DebertaAccuracy.ipynb             # DeBERTa evaluation accuracy
    ├── DebertaFineTuning.ipynb           # DeBERTa fine-tuning for evaluation
    ├── Evaluation100.ipynb               # Main evaluation on 100 samples
    ├── Evaluation500.ipynb               # Main evaluation on 500 samples  
    ├── FlanT5FineTuning.ipynb            # Flan-T5 fine-tuning process
    ├── LabelAccuracycomparingFewShots.ipynb # Few-shot accuracy comparison
    ├── TauKrippendorff100.py             # Kendall's Tau calculation (100 samples)
    └── TauKrippendorff500.py             # Kendall's Tau calculation (500 samples)
```

## Methodology

### Dataset: ACORN
- **3,500 commonsense reasoning explanations** from multiple sources:
  - CoS-E (CommonsenseQA explanations)
  - ECQA (Enhanced CommonsenseQA explanations)  
  - COPA-SSE (BCOPA explanations)
  - GPT-3.5 generated explanations
  - Human-written crowdsourced explanations
- **Multi-dimensional human ratings** across 8 quality criteria
- **140,000 total human ratings** (5 independent ratings per sample)

### Three Modeling Strategies

1. **Fine-tuned Flan-T5 Base** (250M parameters)
   - Trained on 3,000 ACORN samples
   - Text-to-text approach with instruction tuning
   - Optimized for structured explanation generation

2. **Few-shot LLaMA 7B** (7B parameters)  
   - No fine-tuning on ACORN dataset
   - Uses 1-10 few-shot examples at inference time
   - Leverages in-context learning capabilities

3. **Fine-tuned DeBERTaV3 Large** (Evaluation Model)
   - Trained to predict human quality ratings
   - Used for automated evaluation of generated explanations
   - Provides consistent scoring across experiments

### Evaluation Framework

#### Human Evaluation Criteria:
- **Supports**: Does the explanation justify the correct answer?
- **Overall**: General explanation quality (1-5 stars)
- **Well-Written**: Clear, fluent, grammatically correct?
- **Related**: Relevant to the question and answer?
- **Factual**: Are the facts presented correct?
- **New Information**: Does it add insights beyond the question?
- **Unnecessary Information**: Contains redundant statements?
- **Contrastive**: Contrasts different possible solutions?

#### Automated Metrics:
- **Kendall's Tau**: Rank correlation for inter-rater agreement
- **Cosine Similarity**: Semantic alignment using sentence embeddings
- **BERTScore**: Token-level overlap using contextual embeddings
- **BLEURT**: Learned metric trained on human judgments
- **Label Accuracy**: Correct answer prediction based on explanations

## Getting Started

### Prerequisites
- Python 3.8+
- Jupyter Notebook
- Required Python packages (see requirements section below)

### Installation
1. Clone this repository
2. Install required dependencies:
```bash
pip install -r requirements.txt
```

### Running the Code

#### Data Exploration
Start with the data viewer to understand the dataset:
```bash
jupyter notebook Code/theCode/ACORNviewer.ipynb
```

#### Model Training
For DeBERTa:
```bash
jupyter notebook Code/theCode/DebertaFineTuning.ipynb
```

For Flan-T5:
```bash
jupyter notebook Code/theCode/FlanT5FineTuning.ipynb
```

#### Evaluation
Run evaluation notebooks for different sample sizes:
```bash
jupyter notebook Code/theCode/Evaluation100.ipynb
jupyter notebook Code/theCode/Evaluation500.ipynb
```

#### Statistical Analysis
For Krippendorff's Alpha calculation:
```bash
python Code/theCode/TauKrippendorff100.py
python Code/theCode/TauKrippendorff500.py
```

#### Visualization
Generate plots and comparisons:
```bash
python Code/plotCode/plot100.py
python Code/plotCode/plot500.py
```

## Key Findings

### Model Performance Comparison

#### Flan-T5 Strengths:
- **Higher semantic similarity**: Achieves 0.65 cosine similarity vs LLaMA's 0.59
- **Better BERTScore**: 0.86 F1 score vs LLaMA's 0.85  
- **Consistent performance**: Maintains high accuracy (92.2%) across all conditions
- **Structured explanations**: Generates more consistent, training-data-aligned responses
- **Resource efficient**: Smaller model size with strong task-specific performance

#### LLaMA Strengths:  
- **Superior BLEURT scores**: -0.52 vs Flan-T5's -0.96 (less penalty for stylistic differences)
- **Few-shot adaptability**: Performance improves from 62.4% to 91.4% accuracy (1-shot to 10-shot)
- **Complex reasoning**: Better suited for open-ended, nuanced explanations
- **Contextual learning**: Benefits significantly from additional few-shot examples
- **Factual insights**: Shows stronger correlation in providing new information

#### Human-AI Agreement:
- **DeBERTaV3 correlation**: 0.621 Kendall's Tau with aggregated human ratings
- **Individual vs collective**: Automated ratings align better with consensus than individual human judgments
- **Evaluation consistency**: Flan-T5 shows stronger inter-rater consistency for structured criteria
- **Subjective criteria**: Lower agreement on "Well-Written" and "Contrastive" aspects indicates evaluation challenges

### Few-Shot Learning Effects:
- **Diminishing returns**: LLaMA benefits plateau after 7-shot examples
- **Sharp initial gains**: Major improvement from 1-shot (62.4%) to 3-shot (87.0%) accuracy  
- **Optimal range**: 5-7 few-shot examples provide best balance of performance and efficiency

### Practical Implications:
1. **Task-specific applications**: Fine-tuned smaller models excel for well-defined, consistent tasks
2. **General reasoning**: Larger few-shot models better for complex, varied reasoning scenarios
3. **Resource considerations**: Flan-T5 offers efficient alternative for structured explanation tasks
4. **Hybrid approaches**: Combining strengths of both approaches could optimize performance

## Results Data

### Generated Outputs:
- `100_FINAL.csv`: Human and automated evaluation results for 100-sample subset
- `500_FINAL.csv`: Comprehensive evaluation results for 500-sample subset
- `results_labelAccuracy.csv`: Label accuracy comparison across few-shot settings

### Visualizations:
- Kendall's Tau correlation plots for human-AI agreement analysis
- Few-shot performance trends across multiple evaluation metrics
- Label accuracy comparisons showing diminishing returns of additional examples
- Inter-rater agreement visualizations for evaluation consistency

## Dependencies

### Core ML/NLP Libraries:
```txt
torch>=1.9.0
transformers>=4.21.0
datasets>=2.0.0
accelerate>=0.12.0
sentence-transformers>=2.0.0
```

### Evaluation Metrics:
```txt
bert-score>=0.3.10
bleurt @ git+https://github.com/google-research/bleurt.git
scipy>=1.7.0
scikit-learn>=1.0.0
```

### Data Processing & Analysis:
```txt
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.5.0
seaborn>=0.11.0
jupyter>=1.0.0
tabulate>=0.8.0
```

### Additional Requirements:
```txt
tf-keras
ai2-olmo
hf_olmo
```

## Authors

**Aaron Eidt** and **Gana Hamdi**

Technische Universität Berlin - Interdisciplinary Media Project

## Citation

If you use this code or findings in your research, please cite:

```bibtex
@article{eidt2024finetuning,
  title={Fine-Tuning vs. Few-Shot: Evaluating Flan-T5 and LLaMA for Commonsense Reasoning Explanations},
  author={Eidt, Aaron and Hamdi, Gana},
  journal={Interdisciplinary Media Project},
  year={2024},
  institution={Technische Universität Berlin}
}
```

## License

This project is for academic research purposes. The code is provided for reproducibility and educational use. Please cite appropriately if using this methodology or code in your research.

## Acknowledgments

- **ACORN Dataset**: Brassard et al. for providing the comprehensive commonsense reasoning explanation dataset
- **Model Providers**: Hugging Face for Flan-T5 and DeBERTaV3, Meta AI for LLaMA
- **Evaluation Frameworks**: Google Research for BLEURT, various contributors for BERTScore 